{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee677bb1-cc37-4c53-ab47-b5a12865a31f",
   "metadata": {},
   "source": [
    "### Stereo vision\n",
    "\n",
    "See for model \n",
    "https://pytorch.org/vision/stable/models/mask_rcnn.html\n",
    "\n",
    "https://pytorch.org/vision/stable/models/generated/torchvision.models.detection.maskrcnn_resnet50_fpn.html#torchvision.models.detection.maskrcnn_resnet50_fpn\n",
    "\n",
    "See for visualisation https://pytorch.org/vision/stable/auto_examples/plot_visualization_utils.html#sphx-glr-auto-examples-plot-visualization-utils-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c982bead-bd73-45a4-8987-0b224eecf6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as tvtf\n",
    "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights,MaskRCNN_ResNet50_FPN_V2_Weights\n",
    "import stereo_image_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50240608-1150-4622-b7b2-0fe22b75d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are some helper functions\n",
    "#load image from file\n",
    "# preprocess image for input into mask rcnn model\n",
    "# display image\n",
    "# display image pair, to display two images\n",
    "\n",
    "def load_img(filename):\n",
    "    img = cv2.imread(filename)\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = tvtf.to_tensor(image)\n",
    "    image = image.unsqueeze(dim=0)\n",
    "    return image\n",
    "\n",
    "def display_image(image):\n",
    "    fig, axes = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    if image.ndim == 2:\n",
    "        axes.imshow(image, cmap='gray', vmin=0, vmax=255)\n",
    "    else:\n",
    "        axes.imshow(image)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def display_image_pair(first_image, second_image):\n",
    "    #this funciton from Computer vision course notes \n",
    "    # When using plt.subplots, we can specify how many plottable regions we want to create through nrows and ncols\n",
    "    # Here we are creating a subplot with 2 columns and 1 row (i.e. side-by-side axes)\n",
    "    # When we do this, axes becomes a list of length 2 (Containing both plottable axes)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n",
    "    \n",
    "    # TODO: Call imshow on each of the axes with the first and second images\n",
    "    #       Make sure you handle both RGB and grayscale images\n",
    "    if first_image.ndim == 2:\n",
    "        axes[0].imshow(first_image, cmap='gray', vmin=0, vmax=255)\n",
    "    else:\n",
    "        axes[0].imshow(first_image)\n",
    "\n",
    "    if second_image.ndim == 2:\n",
    "        axes[1].imshow(second_image, cmap='gray', vmin=0, vmax=255)\n",
    "    else:\n",
    "        axes[1].imshow(second_image)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364b6893-f00e-4330-8494-a33224bde4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these colours are used to draw boxes.\n",
    "\n",
    "COLOURS = [\n",
    "    tuple(int(colour_hex.strip('#')[i:i+2], 16) for i in (0, 2, 4))\n",
    "    for colour_hex in plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9bd65d-bef4-45d9-9d5c-24132701a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have two images, a left an a right image with my iphone camera. I am holding the camera with my hand\n",
    "# so it is not an exact grid\n",
    "\n",
    "# d_calib = \"40cm\"\n",
    "# d_calib = \"50cm\"\n",
    "d_calib = \"40-72\"\n",
    "\n",
    "path = \"./dl_images/\"\n",
    "\n",
    "left_eye = path + 'imgL_' + d_calib + '.png' #'left_eye_demo2.jpg'\n",
    "right_eye = path + 'imgR_' + d_calib + '.png' #'right_eye_demo2.jpg'\n",
    "\n",
    "# left_eye = path + 'left_eye_' + d_calib + '.jpg' #'left_eye_demo2.jpg'\n",
    "# right_eye = path + 'right_eye_' + d_calib + '.jpg' #'right_eye_demo2.jpg'\n",
    "\n",
    "#down sample image to get same size as expected from esp32 cam\n",
    "\n",
    "\n",
    "left_img = load_img(left_eye)\n",
    "# left_img = cv2.resize(left_img, dsize=(sz1,sz2), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "right_img = load_img(right_eye)\n",
    "# right_img = cv2.resize(right_img, dsize=(sz1,sz2), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "sz1 = right_img.shape[1]\n",
    "sz2 = right_img.shape[0]\n",
    "\n",
    "display_image_pair(left_img, right_img)\n",
    "\n",
    "imgs = [left_img, right_img]\n",
    "\n",
    "left_right = [preprocess_image(d).squeeze(dim=0) for d in imgs]\n",
    "\n",
    "print(right_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82014203-710a-4010-a45b-ffdcdcd73bd6",
   "metadata": {},
   "source": [
    "## do object detection with MaskRCNN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5835fbaa-ab9c-41c8-a978-627f09c629f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the default weights and set up the model\n",
    "\n",
    "weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=weights)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab572319-9c27-4763-9bb1-4ecfcce5413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions returns the detections\n",
    "# det is the boxes, top left and bottom right cooridinates\n",
    "# lbls are the class labels\n",
    "# scores are the confidence. We use 0.5 as default\n",
    "# masks are the segmentation masks.\n",
    "\n",
    "def get_detections(maskrcnn, imgs, score_threshold=0.5): #person, dog, elephan, zebra, giraffe, toilet\n",
    "    ''' Runs maskrcnn over all frames in vid, storing the detections '''\n",
    "    # Record how long the video is (in frames)\n",
    "    det = []\n",
    "    lbls = []\n",
    "    scores = []\n",
    "    masks = []\n",
    "    \n",
    "    for img in imgs:\n",
    "        with torch.no_grad():\n",
    "            result = maskrcnn(preprocess_image(img))[0]\n",
    "    \n",
    "        mask = result[\"scores\"] > score_threshold\n",
    "\n",
    "        boxes = result[\"boxes\"][mask].detach().cpu().numpy()\n",
    "        det.append(boxes)\n",
    "        lbls.append(result[\"labels\"][mask].detach().cpu().numpy())\n",
    "        scores.append(result[\"scores\"][mask].detach().cpu().numpy())\n",
    "#         masks.append(result[\"masks\"][mask].detach().cpu().numpy())\n",
    "        masks.append(result[\"masks\"][mask]) #I want this as a tensor\n",
    "        \n",
    "    # det is bounding boxes, lbls is class labels, scores are confidences and masks are segmentation masks\n",
    "    return det, lbls, scores, masks\n",
    "\n",
    "#det[0] are the bounding boxes in the left image\n",
    "#det[1] are the bounding boxes in the right image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8110b96-9a03-4c94-ae91-a0666974cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "det, lbls, scores, masks = get_detections(model,imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540aac5e-7e01-4ec4-82f9-0bf4f7680b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(weights.meta[\"categories\"])[lbls[0]])\n",
    "print(np.array(weights.meta[\"categories\"])[lbls[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1677f0d6-ae26-4857-b46b-60a2e594c031",
   "metadata": {},
   "source": [
    "### Do some visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c624e181-37dc-4d34-8981-1f50cc7e51d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#draws the bounding boxes\n",
    "def draw_detections(img, det, colours=COLOURS, obj_order = None):\n",
    "    for i, (tlx, tly, brx, bry) in enumerate(det):\n",
    "        if obj_order is not None and len(obj_order) < i:\n",
    "            i = obj_order[i]\n",
    "        i %= len(colours)        \n",
    "        cv2.rectangle(img, (tlx, tly), (brx, bry), color=colours[i], thickness=2)\n",
    "\n",
    "\n",
    "#annotate the class labels\n",
    "def annotate_class(img, det, lbls, conf=None, colours=COLOURS, class_map=weights.meta[\"categories\"]):\n",
    "    for i, ( tlx, tly, brx, bry) in enumerate(det):\n",
    "        txt = class_map[lbls[i]]\n",
    "        if conf is not None:\n",
    "            txt += f' {conf[i]:1.3f}'\n",
    "        # A box with a border thickness draws half of that thickness to the left of the \n",
    "        # boundaries, while filling fills only within the boundaries, so we expand the filled\n",
    "        # region to match the border\n",
    "        offset = 1\n",
    "        \n",
    "        cv2.rectangle(img, \n",
    "                      (tlx-offset, tly-offset+12),\n",
    "                      (tlx-offset+len(txt)*12, tly),\n",
    "                      color=colours[i%len(colours)],\n",
    "                      thickness=cv2.FILLED)\n",
    "        \n",
    "        ff = cv2.FONT_HERSHEY_PLAIN\n",
    "        cv2.putText(img, txt, (tlx, tly-1+12), fontFace=ff, fontScale=1.0, color=(255,)*3)\n",
    "\n",
    "\n",
    "def draw_instance_segmentation_mask(img, masks):\n",
    "    ''' Draws segmentation masks over an img '''\n",
    "    seg_colours = np.zeros_like(img, dtype=np.uint8)\n",
    "    for i, mask in enumerate(masks):\n",
    "        col = (mask[0, :, :, None] * COLOURS[i])\n",
    "        seg_colours = np.maximum(seg_colours, col.astype(np.uint8))\n",
    "    cv2.addWeighted(img, 0.75, seg_colours, 0.75, 1.0, dst=img)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026086a3-f328-497b-a63d-d154a061eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
    "\n",
    "for i, imgi in enumerate(imgs):\n",
    "    img = imgi.copy()\n",
    "    deti = det[i].astype(np.int32)\n",
    "    draw_detections(img,deti)\n",
    "    masksi = masks[i].detach().cpu().numpy()\n",
    "    annotate_class(img,deti,lbls[i])\n",
    "#     draw_instance_segmentation_mask(img, masksi)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'Frame #{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0119cb41-ccf3-4e34-bdf2-fd06e97df701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw with masks\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
    "# imgs2 = imgs.copy()\n",
    "\n",
    "for i, imgi in enumerate(imgs):\n",
    "    img = imgi.copy()\n",
    "    deti = det[i].astype(np.int32)\n",
    "    draw_detections(img,deti)\n",
    "    masks[i][masks[i]<0.7]=0\n",
    "    masksi = masks[i].detach().cpu().numpy()\n",
    "#     annotate_class(img,deti,lbls[i])\n",
    "    draw_instance_segmentation_mask(img, masksi)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'Frame #{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a201569-6666-425f-8b7c-ebf17be7739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get centr, top left and bottom right of boxes\n",
    "\n",
    "def tlbr_to_center1(boxes):\n",
    "    points = []\n",
    "    for tlx, tly, brx, bry in boxes:\n",
    "        cx = (tlx+brx)/2\n",
    "        cy = (tly+bry)/2\n",
    "        points.append([cx, cy])\n",
    "    return points\n",
    "\n",
    "def tlbr_to_corner(boxes):\n",
    "    points = []\n",
    "    for tlx, tly, brx, bry in boxes:\n",
    "        cx = (tlx+tlx)/2\n",
    "        cy = (tly+tly)/2\n",
    "        points.append((cx, cy))\n",
    "    return points\n",
    "\n",
    "def tlbr_to_corner_br(boxes):\n",
    "    points = []\n",
    "    for tlx, tly, brx, bry in boxes:\n",
    "        cx = (brx+brx)/2\n",
    "        cy = (bry+bry)/2\n",
    "        points.append((cx, cy))\n",
    "    return points\n",
    "\n",
    "def tlbr_to_area(boxes):\n",
    "    areas = []\n",
    "    for tlx, tly, brx, bry in boxes:\n",
    "        cx = (brx-tlx)\n",
    "        cy = (bry-tly)\n",
    "        areas.append(abs(cx*cy))\n",
    "    return areas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6edaf99-6d65-426b-95e8-ad5e1be5cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all distances from every object box to every other object box\n",
    "#left image is boxes[0]\n",
    "#right image is boxes[1]\n",
    "\n",
    "#do broad casting.\n",
    "#in python, col vector - row vector gives matrix:\n",
    "# [a] - [c,d] = [a-c, a-d]\n",
    "# [b]           [b-c, b-d]\n",
    "\n",
    "def get_horiz_dist_centre(boxes):\n",
    "    pnts1 = np.array(tlbr_to_center1(boxes[0]))[:,0]\n",
    "    pnts2 = np.array(tlbr_to_center1(boxes[1]))[:,0]\n",
    "    return pnts1[:,None] - pnts2[None]\n",
    "\n",
    "def get_horiz_dist_corner_tl(boxes):\n",
    "    pnts1 = np.array(tlbr_to_corner(boxes[0]))[:,0]\n",
    "    pnts2 = np.array(tlbr_to_corner(boxes[1]))[:,0]\n",
    "    return pnts1[:,None] - pnts2[None]\n",
    "\n",
    "def get_horiz_dist_corner_br(boxes):\n",
    "    pnts1 = np.array(tlbr_to_corner_br(boxes[0]))[:,0]\n",
    "    pnts2 = np.array(tlbr_to_corner_br(boxes[1]))[:,0]\n",
    "    return pnts1[:,None] - pnts2[None]\n",
    "\n",
    "def get_vertic_dist_centre(boxes):\n",
    "    pnts1 = np.array(tlbr_to_center1(boxes[0]))[:,1]\n",
    "    pnts2 = np.array(tlbr_to_center1(boxes[1]))[:,1]\n",
    "    return pnts1[:,None] - pnts2[None]\n",
    "\n",
    "def get_area_diffs(boxes):\n",
    "    pnts1 = np.array(tlbr_to_area(boxes[0]))\n",
    "    pnts2 = np.array(tlbr_to_area(boxes[1]))\n",
    "    return abs(pnts1[:,None] - pnts2[None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85405a00-f27e-4f4e-8bc7-e5ea01f92d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get distance bentween corner and centre\n",
    "\n",
    "centre = sz1/2\n",
    "\n",
    "def get_dist_to_centre_tl(box, cntr = centre):\n",
    "    pnts = np.array(tlbr_to_corner(box))[:,0]\n",
    "    return abs(pnts - cntr)\n",
    "\n",
    "\n",
    "def get_dist_to_centre_br(box, cntr = centre):\n",
    "    pnts = np.array(tlbr_to_corner_br(box))[:,0]\n",
    "    return abs(pnts - cntr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c78093b-7497-4884-ad8e-0ae57d757a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = get_dist_to_centre_br(det[0])\n",
    "tmp2 = get_dist_to_centre_br(det[1])\n",
    "print(tmp1)\n",
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c4579-1349-4599-aeac-47d6b9406092",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the tracking cost function.\n",
    "#consists of theree parts.\n",
    "#  1. The vertical move up and down of object centre of mass. Scale this up because we do not expect this to be very much.\n",
    "#  2. The move left or right by the object. We only expect it to move right (from the left eye image). So penalise if it moves left.\n",
    "#  3. The difference in area of pixels. Area of image is width x height, so divide by height, there for this will have max value of width\n",
    "\n",
    "def get_cost(boxes, lbls = None, sz1 = 640):\n",
    "    alpha = sz1; beta  = 10; gamma = 5\n",
    "    \n",
    "    #vertical_dist, scale by gamma since can't move up or down\n",
    "    vert_dist = gamma*abs(get_vertic_dist_centre(boxes))\n",
    "    \n",
    "    #horizonatl distance.\n",
    "    horiz_dist = get_horiz_dist_centre(boxes)\n",
    "    \n",
    "    #increase cost if object has moved from right to left.\n",
    "    horiz_dist[horiz_dist<0] = beta*abs(horiz_dist[horiz_dist<0])\n",
    "    \n",
    "    #area of box\n",
    "    area_diffs = get_area_diffs(boxes)/alpha\n",
    "    \n",
    "    cost = np.array([vert_dist,horiz_dist,area_diffs])\n",
    "    \n",
    "    cost=cost.sum(axis=0)\n",
    "    \n",
    "    #add penalty term for different object classes\n",
    "    if lbls is not None:\n",
    "        for i in range(cost.shape[0]):\n",
    "            for j in range(cost.shape[1]):\n",
    "                if lbls[0][i] != lbls[1][j]:\n",
    "                    cost[i,j]+=150\n",
    "    return cost\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0556d8b-2dd9-495e-9d3d-8fd0ba23bd7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get cost with centre of mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb53580-2a22-451a-a5f7-513a49b06621",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cost_with_com(masks, lbls = None, prob_thresh = 0.7):\n",
    "    alpha = 480; beta  = 10; gamma = 5\n",
    "\n",
    "    #left masks\n",
    "    mask_bool = masks[0] > prob_thresh\n",
    "    mask_bool = mask_bool.squeeze(1)\n",
    "    #right masks\n",
    "    mask_bool2 = masks[1] > prob_thresh\n",
    "    mask_bool2 = mask_bool2.squeeze(1)\n",
    "    \n",
    "    #left params\n",
    "    #com1 is center of mass of height\n",
    "    #com2 is center of mass of width\n",
    "    mask_size = (mask_bool).sum(dim=[1,2])\n",
    "    mask_com_matrix_1 = torch.tensor(range(mask_bool.shape[1]))\n",
    "    com1 = ((mask_com_matrix_1.unsqueeze(1))*mask_bool).sum(dim=[1,2])/mask_size\n",
    "    mask_com_matrix_2 = torch.tensor(range(mask_bool.shape[2]))\n",
    "    com2 = ((mask_com_matrix_2.unsqueeze(0))*mask_bool).sum(dim=[1,2])/mask_size\n",
    "\n",
    "    left_params = torch.stack((com1, com2, mask_size)).transpose(1,0)\n",
    "    \n",
    "    #get right params\n",
    "    mask_size2 = (mask_bool2).sum(dim=[1,2])\n",
    "    mask_com_matrix_12 = torch.tensor(range(mask_bool2.shape[1]))\n",
    "    com12 = ((mask_com_matrix_12.unsqueeze(1))*mask_bool2).sum(dim=[1,2])/mask_size2\n",
    "    mask_com_matrix_22 = torch.tensor(range(mask_bool2.shape[2]))\n",
    "    com22 = ((mask_com_matrix_22.unsqueeze(0))*mask_bool2).sum(dim=[1,2])/mask_size2\n",
    "\n",
    "    right_params = torch.stack((com12, com22, mask_size2)).transpose(1,0)\n",
    "    \n",
    "    #calculate cost function\n",
    "    cost = (left_params[:,None] - right_params[None])\n",
    "    #scale counts\n",
    "    cost[:,:,2]=abs(cost[:,:,2])/alpha\n",
    "\n",
    "    #can't move right, can only move left\n",
    "    cost[cost[:,:,1]<0] = beta*abs(cost[cost[:,:,1]<0])\n",
    "\n",
    "    #move up and down, take abs vals\n",
    "    cost[:,:,0] = gamma*abs(cost[:,:,0])\n",
    "    # print(cost.shape)\n",
    "    cost = cost.sum(dim=2)\n",
    "    if lbls is not None:\n",
    "        for i in range(cost.shape[0]):\n",
    "            for j in range(cost.shape[1]):\n",
    "                if lbls[0][i] != lbls[1][j] :\n",
    "                    cost[i,j]+=100\n",
    "#                     print(lbls[0][i], lbls[1][j])\n",
    "    return cost\n",
    "   \n",
    "    \n",
    "def get_horiz_dist(masks, prob_thresh = 0.7):\n",
    "    # gets the horizontal distance between the centre of mass for each object\n",
    "    #left masks\n",
    "    mask_bool = masks[0] > prob_thresh\n",
    "    mask_bool = mask_bool.squeeze(1)\n",
    "    #right masks\n",
    "    mask_bool2 = masks[1] > prob_thresh\n",
    "    mask_bool2 = mask_bool2.squeeze(1)\n",
    "    \n",
    "    #left params\n",
    "    #com1 is center of mass of height\n",
    "    #com2 is center of mass of width\n",
    "    mask_size = (mask_bool).sum(dim=[1,2])\n",
    "    mask_com_matrix_1 = torch.tensor(range(mask_bool.shape[1]))\n",
    "    com1 = ((mask_com_matrix_1.unsqueeze(1))*mask_bool).sum(dim=[1,2])/mask_size\n",
    "    mask_com_matrix_2 = torch.tensor(range(mask_bool.shape[2]))\n",
    "    com2 = ((mask_com_matrix_2.unsqueeze(0))*mask_bool).sum(dim=[1,2])/mask_size\n",
    "\n",
    "    left_params = torch.stack((com1, com2, mask_size)).transpose(1,0)\n",
    "    \n",
    "    #get right params\n",
    "    mask_size2 = (mask_bool2).sum(dim=[1,2])\n",
    "    mask_com_matrix_12 = torch.tensor(range(mask_bool2.shape[1]))\n",
    "    com12 = ((mask_com_matrix_12.unsqueeze(1))*mask_bool2).sum(dim=[1,2])/mask_size2\n",
    "    mask_com_matrix_22 = torch.tensor(range(mask_bool2.shape[2]))\n",
    "    com22 = ((mask_com_matrix_22.unsqueeze(0))*mask_bool2).sum(dim=[1,2])/mask_size2\n",
    "\n",
    "    right_params = torch.stack((com12, com22, mask_size2)).transpose(1,0)\n",
    "    \n",
    "    #calculate cost function\n",
    "    cost = (left_params[:,None] - right_params[None])\n",
    "    return cost[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9027765-1eec-4002-bbeb-ed2ef6de025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tracks(cost):\n",
    "    return scipy.optimize.linear_sum_assignment(cost)\n",
    "    \n",
    "\n",
    "def get_tracks_ij(cost):\n",
    "    tracks = scipy.optimize.linear_sum_assignment(cost)\n",
    "    return [[i,j] for i, j in zip(*tracks)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf2e50d-4435-4153-937f-641c0ac8eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = get_cost(det, lbls = lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524fc8fb-895a-4dc5-a666-3f6d818e3c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = scipy.optimize.linear_sum_assignment(cost)\n",
    "\n",
    "h_d = [[np.array(weights.meta[\"categories\"])[lbls[0]][i],np.array(weights.meta[\"categories\"])[lbls[1]][j]] for i, j in zip(*tracks)]\n",
    "print(np.array(weights.meta[\"categories\"])[lbls[0]])\n",
    "print(h_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d246be9-57d9-4724-9709-61dc41183173",
   "metadata": {},
   "source": [
    "## the distance between the left and right centres is given by dists for each object.\n",
    "\n",
    "##### We use the geometric formulas to calibrate the tantheta and focal lenght of the camera for 30cm and 50cm bottle and then transform the pixels differences to distance in cm's for each object in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b64b6dc-ddae-41b6-9ae1-f76d8cd766d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we take the corner that is closest to the centre. This is because the other corner might be going off the image.\n",
    "\n",
    "dists_tl =  get_horiz_dist_corner_tl(det)\n",
    "dists_br =  get_horiz_dist_corner_br(det)\n",
    "\n",
    "final_dists = []\n",
    "dctl = get_dist_to_centre_tl(det[0])\n",
    "dcbr = get_dist_to_centre_br(det[0])\n",
    "\n",
    "for i, j in zip(*tracks):\n",
    "    if dctl[i] < dcbr[i]:\n",
    "        final_dists.append((dists_tl[i][j],np.array(weights.meta[\"categories\"])[lbls[0]][i]))\n",
    "        \n",
    "    else:\n",
    "        final_dists.append((dists_br[i][j],np.array(weights.meta[\"categories\"])[lbls[0]][i]))\n",
    "        \n",
    "\n",
    "final_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45d6791-72da-485d-8834-f953d5017961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get focal length\n",
    "# for 50cm away bottle image, we had -106.21 pixels between bottle boxes \n",
    "# and for 40cm away bottle image we had 86.28 pixels between left and right bottles\n",
    "fl = 40+336.175*50/-276.542\n",
    "print(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5de80c-89e6-4540-96fa-bc963a288479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calibrate theta. cameras are 15 cms apart\n",
    "tantheta = (1/(50-abs(fl)))*(15.0/2)*sz1/-276.542\n",
    "print(tantheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c43360-d7f3-4405-8b57-661fe19f7387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final distances as list\n",
    "fd = [i for (i,j) in final_dists]\n",
    "print(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fabfab-9065-40cc-9a42-d951d16c5b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find distance away\n",
    "dists_away = (15.0/2)*sz1*(1/tantheta)/np.array([abs(fd[0])])+fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ece84-c7ec-41c9-9752-2f475ec40a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dist = []\n",
    "for i in range(len(dists_away)):\n",
    "    cat_dist.append(f'{np.array(weights.meta[\"categories\"])[lbls[0]][i]} {dists_away[i]:.1f}cm')\n",
    "    print(f'{np.array(weights.meta[\"categories\"])[lbls[0]][i]} is {dists_away[i]:.2f}cm away')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b59ed9e-c352-4667-b0bb-c4fce1e634bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#annotate the class labels\n",
    "def annotate_class2(img, det, lbls, class_map, conf=None,  colours=COLOURS):\n",
    "    for i, ( tlx, tly, brx, bry) in enumerate(det):\n",
    "        txt = class_map[i]\n",
    "        if conf is not None:\n",
    "            txt += f' {conf[i]:1.3f}'\n",
    "        # A box with a border thickness draws half of that thickness to the left of the \n",
    "        # boundaries, while filling fills only within the boundaries, so we expand the filled\n",
    "        # region to match the border\n",
    "        offset = 1\n",
    "        \n",
    "        cv2.rectangle(img, \n",
    "                      (tlx-offset, tly-offset+12),\n",
    "                      (tlx-offset+len(txt)*12, tly),\n",
    "                      color=colours[i%len(colours)],\n",
    "                      thickness=cv2.FILLED)\n",
    "        \n",
    "        ff = cv2.FONT_HERSHEY_PLAIN\n",
    "        cv2.putText(img, txt, (tlx, tly-1+12), fontFace=ff, fontScale=1.0, color=(255,)*3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6112071-a7c5-4a83-81f4-6200c7339897",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.array(cat_dist)[(tracks[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50399877-bebd-4841-b2c3-072a39e7b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
    "\n",
    "t1 = [list(tracks[0]), list(tracks[1])]\n",
    "\n",
    "for i, imgi in enumerate(imgs):\n",
    "    img = imgi.copy()\n",
    "    deti = det[i].astype(np.int32)\n",
    "    draw_detections(img,deti[list(tracks[i])], obj_order=list(t1[i]))\n",
    "    annotate_class2(img,deti[list(tracks[i])],lbls[i][list(tracks[i])],cat_dist)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'Frame #{i}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad11aae-9660-4394-87f3-fc013f019879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
